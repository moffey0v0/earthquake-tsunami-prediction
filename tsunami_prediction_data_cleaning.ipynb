{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f5d754-4b4a-449b-a2f6-0c646e5b9369",
   "metadata": {},
   "source": [
    "Part 1: Data Fetching from USGS\n",
    "This script fetches earthquake data (both tsunami and non-tsunami events) from the USGS API and saves it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31787cd-c9ee-489e-89e9-851495f84f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_usgs_mixed_data(min_magnitude=6.0, start_year=2000):\n",
    "    \"\"\"\n",
    "    Fetch mixed data (positive/tsunami and negative/no-tsunami samples) from USGS.\n",
    "    Uses the GeoJSON interface to retrieve detailed fields like cdi, mmi, sig.\n",
    "    \"\"\"\n",
    "    # USGS API endpoint\n",
    "    url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "    \n",
    "    # Parameters: Fetch all earthquakes > min_magnitude from the past 20+ years\n",
    "    # This ensures a mix of large earthquakes that did and did not cause tsunamis.\n",
    "    params = {\n",
    "        \"format\": \"geojson\",\n",
    "        \"starttime\": f\"{start_year}-01-01\",\n",
    "        \"minmagnitude\": min_magnitude,\n",
    "        \"orderby\": \"time\",\n",
    "        \"limit\": 20000  # Fetch a sufficient amount of data\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching all earthquakes > {min_magnitude} from {start_year} to present from USGS...\")\n",
    "    print(\"This includes both 'tsunami-generating' and 'non-tsunami' events...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        features = data.get('features', [])\n",
    "        print(f\"Successfully retrieved {len(features)} earthquake records.\")\n",
    "        \n",
    "        # Parse GeoJSON data to match the desired CSV format\n",
    "        parsed_data = []\n",
    "        for feature in features:\n",
    "            props = feature['properties']\n",
    "            geometry = feature['geometry']['coordinates'] # [long, lat, depth]\n",
    "            \n",
    "            # Convert timestamp\n",
    "            timestamp = props.get('time')\n",
    "            if timestamp:\n",
    "                dt = datetime.fromtimestamp(timestamp / 1000)\n",
    "                year = dt.year\n",
    "                month = dt.month\n",
    "            else:\n",
    "                year, month = np.nan, np.nan\n",
    "\n",
    "            # Extract Tsunami flag (This is the Target)\n",
    "            # In USGS: 1 = Tsunami, 0 or null = No Tsunami\n",
    "            tsunami_flag = props.get('tsunami')\n",
    "            if tsunami_flag is None:\n",
    "                tsunami_flag = 0\n",
    "            \n",
    "            # Construct row data (strictly matching required column names)\n",
    "            row = {\n",
    "                'magnitude': props.get('mag'),\n",
    "                'cdi': props.get('cdi'),       \n",
    "                'mmi': props.get('mmi'),       \n",
    "                'sig': props.get('sig'),       \n",
    "                'nst': props.get('nst'),\n",
    "                'dmin': props.get('dmin'),\n",
    "                'gap': props.get('gap'),\n",
    "                'depth': geometry[2],          # Depth is the 3rd coordinate\n",
    "                'latitude': geometry[1],\n",
    "                'longitude': geometry[0],\n",
    "                'Year': year,\n",
    "                'Month': month,\n",
    "                'tsunami': int(tsunami_flag)   # Target column: 0 or 1\n",
    "            }\n",
    "            parsed_data.append(row)\n",
    "            \n",
    "        df = pd.DataFrame(parsed_data)\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Data fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Fetch data\n",
    "    df_mixed = fetch_usgs_mixed_data(min_magnitude=6.0) \n",
    "    \n",
    "    if df_mixed is not None:\n",
    "        # 2. Select target columns\n",
    "        target_columns = [\n",
    "            'magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', \n",
    "            'depth', 'latitude', 'longitude', 'Year', 'Month', 'tsunami'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all columns exist\n",
    "        df_final = df_mixed[target_columns]\n",
    "        \n",
    "        # 3. Print statistics\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"Dataset Statistics (Class Distribution)\")\n",
    "        print(\"=\"*30)\n",
    "        print(df_final['tsunami'].value_counts().rename({0: 'No Tsunami (Negative)', 1: 'Tsunami (Positive)'}))\n",
    "        \n",
    "        # 4. Save\n",
    "        filename = \"usgs_earthquake_tsunami_mixed.csv\"\n",
    "        df_final.to_csv(filename, index=False)\n",
    "        print(f\"\\nFile saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb42b0-db77-49f3-ada2-33972fc34bc6",
   "metadata": {},
   "source": [
    "Part 2: Data Cleaning (Filtering & Filling)\n",
    "This script reads the raw data generated in Part 1, filters it for the years 2013-2025, fills missing values with 0, and saves the cleaned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4163960-8623-4218-8834-f381b6b40ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Define filenames\n",
    "input_file = 'usgs_earthquake_tsunami_mixed.csv'\n",
    "output_file = 'usgs_earthquake_tsunami_mixed_2013_2025_filled.csv'\n",
    "\n",
    "# 2. Check if input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"❌ Error: Input file '{input_file}' not found.\")\n",
    "    print(\"Please ensure the file from Part 1 exists in the directory.\")\n",
    "else:\n",
    "    # 3. Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Original row count: {len(df)}\")\n",
    "\n",
    "    # 4. Filter by Year (2013 - 2025)\n",
    "    df_filtered = df[(df['Year'] >= 2013) & (df['Year'] <= 2025)].copy()\n",
    "    print(f\"Row count after 2013-2025 filtering: {len(df_filtered)}\")\n",
    "\n",
    "    # 5. Fill missing values (NaN) with 0\n",
    "    df_filled = df_filtered.fillna(0)\n",
    "    print(\"Filled all missing values (NaN) with 0.\")\n",
    "\n",
    "    # 6. Save processed file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"✅ Success! File generated: {output_file}\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f78da0-a60a-4166-a432-c60ef15f73c3",
   "metadata": {},
   "source": [
    "Part 3: Data Deduplication (Removing Shadow Records)\n",
    "This script takes the cleaned data from Part 2, identifies and removes extremely similar \"shadow records\" (same time, exact magnitude, very close location/depth), and saves the final deduplicated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a5859-e59c-48fe-9a79-ecdad2e0f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_duplicates():\n",
    "    input_file = 'usgs_earthquake_tsunami_mixed_2013_2025_filled.csv'\n",
    "    output_dups = 'extremely_similar_records.csv'\n",
    "    output_clean = 'usgs_earthquake_tsunami_mixed_2013_2025_deduplicated.csv'\n",
    "    \n",
    "    print(f\"Loading file: {input_file} ...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found.\")\n",
    "        return\n",
    "\n",
    "    # Add temporary ID for tracking\n",
    "    df['temp_id'] = df.index\n",
    "    \n",
    "    # Self-join to find potential duplicates\n",
    "    # Criteria: Same Year, Month, and Exact Magnitude\n",
    "    merged = pd.merge(df, df, on=['Year', 'Month', 'magnitude'], suffixes=('_1', '_2'))\n",
    "    \n",
    "    # Filter out self-matches and duplicate pairs (keep index_1 < index_2 only)\n",
    "    merged = merged[merged['temp_id_1'] < merged['temp_id_2']]\n",
    "    \n",
    "    # Apply strict physical thresholds for \"Extremely Similar\"\n",
    "    # Latitude/Longitude diff < 1.0 degree\n",
    "    # Depth diff < 10.0 km\n",
    "    duplicates = merged[\n",
    "        (np.abs(merged['latitude_1'] - merged['latitude_2']) < 1.0) &\n",
    "        (np.abs(merged['longitude_1'] - merged['longitude_2']) < 1.0) &\n",
    "        (np.abs(merged['depth_1'] - merged['depth_2']) < 10.0)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Detected {len(duplicates)} pairs of extremely similar data.\")\n",
    "    \n",
    "    # 1. Save these potential duplicates for inspection\n",
    "    ids_involved = set(duplicates['temp_id_1']).union(set(duplicates['temp_id_2']))\n",
    "    df_dups = df[df['temp_id'].isin(ids_involved)].copy()\n",
    "    df_dups = df_dups.sort_values(by=['Year', 'Month', 'magnitude'])\n",
    "    df_dups.drop(columns=['temp_id'], inplace=True)\n",
    "    df_dups.to_csv(output_dups, index=False)\n",
    "    print(f\"-> Saved suspected duplicates to: {output_dups} ({len(df_dups)} rows)\")\n",
    "    \n",
    "    # 2. Generate final clean data\n",
    "    # Strategy: For every pair (A, B), keep A (temp_id_1) and drop B (temp_id_2)\n",
    "    ids_to_drop = set(duplicates['temp_id_2'])\n",
    "    df_clean = df[~df['temp_id'].isin(ids_to_drop)].copy()\n",
    "    df_clean.drop(columns=['temp_id'], inplace=True)\n",
    "    \n",
    "    print(f\"-> Original row count: {len(df)}\")\n",
    "    print(f\"-> Deduplicated row count: {len(df_clean)}\")\n",
    "    print(f\"-> Removed {len(ids_to_drop)} redundant records.\")\n",
    "    \n",
    "    df_clean.to_csv(output_clean, index=False)\n",
    "    print(f\"-> Saved final deduplicated data to: {output_clean}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
